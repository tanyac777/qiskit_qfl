{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum kernel for IRIS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit_algorithms.state_fidelities import ComputeUncompute\n",
    "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
    "\n",
    "iris_data = load_iris()\n",
    "\n",
    "features = iris_data.data\n",
    "labels = iris_data.target\n",
    "\n",
    "features = MinMaxScaler().fit_transform(features)\n",
    "\n",
    "algorithm_globals.random_seed = 123\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features, labels, train_size=0.8, random_state=algorithm_globals.random_seed\n",
    ")\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18706\\AppData\\Local\\Temp\\ipykernel_12496\\3210571187.py:3: DeprecationWarning: The class ``qiskit.primitives.sampler.Sampler`` is deprecated as of qiskit 1.2. It will be removed no earlier than 3 months after the release date. All implementations of the `BaseSamplerV1` interface have been deprecated in favor of their V2 counterparts. The V2 alternative for the `Sampler` class is `StatevectorSampler`.\n",
      "  sampler = Sampler()\n"
     ]
    }
   ],
   "source": [
    "adhoc_feature_map = ZZFeatureMap(feature_dimension=features.shape[1], reps=3, entanglement=\"linear\")\n",
    "\n",
    "sampler = Sampler()\n",
    "\n",
    "fidelity = ComputeUncompute(sampler=sampler)\n",
    "\n",
    "adhoc_kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=adhoc_feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel as a callable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callable kernel classification test score: 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "adhoc_svc = SVC(kernel=adhoc_kernel.evaluate)\n",
    "\n",
    "adhoc_svc.fit(train_features, train_labels)\n",
    "\n",
    "adhoc_score_callable_function = adhoc_svc.score(test_features, test_labels)\n",
    "\n",
    "\n",
    "print(f\"Callable kernel classification test score: {adhoc_score_callable_function}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [1.         0.71428571 0.9       ]\n",
      "Recall: [1.         0.83333333 0.81818182]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "pred_labels = adhoc_svc.predict(test_features)\n",
    "assert test_labels.ndim == 1, \"test_labels should be a 1D array\"\n",
    "precision = precision_score(test_labels, pred_labels,average=None)\n",
    "print(f'Precision: {precision}')\n",
    "recall = recall_score(test_labels, pred_labels,average=None)\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          1.          0.29903061  1.\n",
      "   0.17679839  0.53537055  0.67202504  1.          1.          1.\n",
      "   0.26848755  0.3568972  -0.00400029 -0.60391054 -0.44985308 -0.88017868\n",
      "  -0.         -0.16662943 -0.         -0.         -0.         -0.\n",
      "  -0.15394274 -0.         -0.         -0.88746627 -0.50756799 -0.92156448\n",
      "  -0.         -0.         -0.44768897 -1.         -0.46790475 -0.8024289\n",
      "  -0.01547321 -0.         -0.9426871  -0.51617698 -0.2911421  -0.\n",
      "  -0.62202322 -0.         -0.         -0.         -0.24518212 -0.\n",
      "  -0.         -0.91389171 -0.         -0.00774728 -0.         -0.\n",
      "  -0.         -0.         -0.         -0.50273033 -0.26408939 -1.\n",
      "  -1.         -0.27804187]\n",
      " [ 0.1927474   0.15971317  0.18813098  0.          0.23925721  1.\n",
      "   0.18111358  0.04556843  0.82964689  0.96014734  1.          0.7873871\n",
      "   0.          1.          1.          0.          1.          0.15764648\n",
      "   1.          1.          0.84186201  1.          1.          1.\n",
      "   0.          1.          1.          0.          0.          1.\n",
      "   1.          1.          0.18032184  1.          0.          1.\n",
      "   0.70765671  1.         -1.         -0.58450271 -0.         -0.44547772\n",
      "  -0.21505318 -1.         -1.         -1.         -0.85380837 -1.\n",
      "  -1.         -0.55179482 -1.         -0.         -1.         -1.\n",
      "  -0.33313667 -0.14532316 -1.         -1.         -1.         -0.75839041\n",
      "  -1.         -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(adhoc_svc.dual_coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "def epsilon_greedy_bandit_data(model,epsilon):\n",
    "    arms_num = 3\n",
    "    #模型预测值\n",
    "    pred_action = model.predict(train_features)\n",
    "    # get predicted vector in logits way\n",
    "    logits = sampler_qnn.forward(train_features, model.weights)\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "    prob_list = []\n",
    "    arr = [0,1,2]\n",
    "    print(\"Inference Started\")\n",
    "    for i in range(len(pred_action)):\n",
    "        t =  random.random()\n",
    "        if t < epsilon:\n",
    "            remaining_action = [x for x in arr if x != pred_action[i]]\n",
    "            action = random.choice(remaining_action)\n",
    "            #print(action)\n",
    "        else:\n",
    "            action = pred_action[i]\n",
    "            #print(action)\n",
    "        action_list.append(action)\n",
    "\n",
    "        if action == train_labels[i]:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        reward_list.append(reward)\n",
    "\n",
    "        action_match_pred = (action_list[i]==pred_action[i])\n",
    "        per_action_epsilon = epsilon / tf.cast(arms_num, dtype=tf.float32)\n",
    "        prob = (1 - epsilon + per_action_epsilon) * action_match_pred + (\n",
    "          1.0 - action_match_pred) * per_action_epsilon\n",
    "        prob_list.append(prob)\n",
    "    print(\"Inference Completed\")\n",
    "    \n",
    "    action_logits = tf.gather(logits, action_list, axis=1, batch_dims=1).numpy()\n",
    "    weight_scale = tf.math.minimum(\n",
    "          1 - epsilon + per_action_epsilon, per_action_epsilon\n",
    "      )\n",
    "    new_y = collections.OrderedDict([\n",
    "          ('label', pred_action),\n",
    "          ('action', action_list),\n",
    "          ('reward', reward_list),\n",
    "          ('prob', prob_list),\n",
    "          ('logits', action_logits),\n",
    "          ('weight_scale', weight_scale),\n",
    "      ])\n",
    "    bandits_data = collections.OrderedDict(x=train_features, y=new_y)\n",
    "    \n",
    "    return bandits_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def falcon_bandit_data(data,model,mu,gamma):\n",
    "  train_sequences = [data_point[\"sequence\"] for data_point in data]\n",
    "  train_labels = [data_point[\"label\"] for data_point in data]\n",
    "  # Convert the lists to NumPy arrays\n",
    "  train_sequences = np.array(train_sequences)\n",
    "  train_labels = np.array(train_labels)\n",
    "\n",
    "  pred_action = model.predict(train_sequences)\n",
    "  pred_logits = sampler_qnn.forward(train_sequences, model.weights)\n",
    "  #print(\"pred_logits:\",pred_logits)\n",
    "  batch_size = tf.shape(pred_logits)[0]\n",
    "  num_arms = tf.shape(pred_logits)[1]\n",
    "  greedy_action = tf.argmax(pred_logits, axis=1, output_type=tf.int32)\n",
    "  #print(\"greedy_action:\",greedy_action)\n",
    "  greedy_logits = tf.gather(pred_logits, greedy_action, axis=1, batch_dims=1)\n",
    "  #print(\"greedy_logits:\",greedy_logits)\n",
    "  nongreedy_res = tf.expand_dims(greedy_logits, axis=1) - pred_logits\n",
    "  #print(\"nongreedy_res:\",nongreedy_res)\n",
    "  nongreedy_prob = 1.0 / (mu + gamma * nongreedy_res)\n",
    "  #print(\"nongreedy_prob:\",nongreedy_prob)\n",
    "  greedy_indicator = tf.one_hot(\n",
    "      greedy_action, depth=num_arms, on_value=True, off_value=False\n",
    "  )\n",
    "  #print(\"greedy_indicator\",greedy_indicator)\n",
    "  nongreedy_prob = tf.where(\n",
    "      greedy_indicator, tf.zeros_like(pred_logits), nongreedy_prob\n",
    "  )\n",
    "  #print(\"nongreedy_prob\",nongreedy_prob)\n",
    "  # Normalize nongreedy_prob if the sum of probablility is larger than 1 so that\n",
    "  # it is still a probability distribution to be sampled even if mu and gamma\n",
    "  # are not properly set.\n",
    "  nongreedy_prob = nongreedy_prob / tf.math.maximum(\n",
    "      1.0, tf.math.reduce_sum(nongreedy_prob, axis=1, keepdims=True)\n",
    "  )\n",
    "  greedy_prob = 1.0 - tf.math.reduce_sum(nongreedy_prob, axis=1)\n",
    "  greedy_idx = tf.stack([tf.range(batch_size), greedy_action], axis=1)\n",
    "  prob = (\n",
    "      tf.scatter_nd(greedy_idx, greedy_prob, shape=tf.shape(pred_logits))\n",
    "      + nongreedy_prob\n",
    "  )\n",
    "  cumsum_prob = tf.math.cumsum(prob, axis=1)\n",
    "  cumsum_prob = tf.cast(cumsum_prob, dtype=tf.float32)\n",
    "  # We can use tf.random.uniform for independent noise on clients, see\n",
    "  # https://www.tensorflow.org/federated/tutorials/random_noise_generation\n",
    "  random_val = tf.random.uniform(\n",
    "      shape=[batch_size], minval=0, maxval=1, dtype=tf.float32\n",
    "  )\n",
    "  less_idx = tf.math.less(tf.expand_dims(random_val, axis=1), cumsum_prob)\n",
    "  # The `action_helper` is constructed so that if the `cumsum_prob` is larger\n",
    "  # than or equal to `random_val`, the values are the action index; if\n",
    "  # `cumsum_prob` is smaller, the values are a constant number of the largest\n",
    "  # possible index of action. When taking the min of `action_helper`,\n",
    "  # the action where random_val falls in the `cumsum_prob` backet is returned;\n",
    "  # i.e., the action is sampled based on the falcon `prob`.\n",
    "  action_helper = tf.where(\n",
    "      less_idx,\n",
    "      tf.broadcast_to(\n",
    "          tf.range(num_arms, dtype=tf.int32), shape=[batch_size, num_arms]\n",
    "      ),\n",
    "      (num_arms - 1) * tf.ones(shape=(batch_size, num_arms), dtype=tf.int32),\n",
    "  )\n",
    "\n",
    "  # bandit data\n",
    "  action = tf.math.reduce_min(action_helper, axis=1).numpy()\n",
    "  print(\"action:\", action)\n",
    "  action_prob = tf.gather(prob, action, axis=1, batch_dims=1).numpy()\n",
    "  action_logits = tf.gather(pred_logits, action, axis=1, batch_dims=1).numpy()\n",
    "  reward_list = []\n",
    "  for i in range(batch_size):\n",
    "    if action[i] == train_labels[i]:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "    reward_list.append(reward)\n",
    "  #print(\"reward:\",reward_list)\n",
    "  weight_scale = 1.0 / (mu + gamma)\n",
    "  new_y = collections.OrderedDict([\n",
    "          ('label', pred_action),#predicted action\n",
    "          ('action', action),#actual action\n",
    "          ('reward', reward_list),#comparation between correct action(test_label) and actual action\n",
    "          ('prob', action_prob),\n",
    "          ('logits', action_logits),\n",
    "          ('weight_scale', weight_scale),\n",
    "      ])\n",
    "  return collections.OrderedDict(x=train_sequences, y=new_y)\n",
    "\n",
    "\n",
    "#bandits_data = falcon_bandit_data(qnn,0.5,10)\n",
    "#print(\"----------------------------------------------------------------------\")\n",
    "#print(bandits_data['y']['prob'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_sampling_bandit_data(model,temperature):\n",
    "    pred_action = model.predict(train_features)\n",
    "    pred_logits = sampler_qnn.forward(train_features, model.weights)\n",
    "    batch_size = tf.shape(pred_logits)[0]\n",
    "    num_arms = tf.shape(pred_logits)[1]\n",
    "    prob = tf.keras.activations.softmax(pred_logits/temperature, axis=-1)\n",
    "    cumsum_prob = tf.math.cumsum(prob, axis=1)\n",
    "    cumsum_prob = tf.cast(cumsum_prob, dtype=tf.float32)\n",
    "    # We can use tf.random.uniform for independent noise on clients, see\n",
    "    # https://www.tensorflow.org/federated/tutorials/random_noise_generation\n",
    "    random_val = tf.random.uniform(\n",
    "        shape=[batch_size], minval=0, maxval=1, dtype=tf.float32\n",
    "    )\n",
    "    less_idx = tf.math.less(tf.expand_dims(random_val, axis=1), cumsum_prob)\n",
    "    # The `action_helper` is constructed so that if the `cumsum_prob` is larger\n",
    "    # than or equal to `random_val`, the values are the action index; if\n",
    "    # `cumsum_prob` is smaller, the values are a constant number of the largest\n",
    "    # possible index of action. When taking the min of `action_helper`,\n",
    "    # the action where random_val falls in the `cumsum_prob` backet is returned;\n",
    "    # i.e., the action is sampled based on the softmax `prob`.\n",
    "    action_helper = tf.where(\n",
    "        less_idx,\n",
    "        tf.broadcast_to(\n",
    "            tf.range(num_arms, dtype=tf.int32), shape=[batch_size, num_arms]\n",
    "        ),\n",
    "        (num_arms - 1) * tf.ones(shape=(batch_size, num_arms), dtype=tf.int32),\n",
    "    )\n",
    "    action = tf.math.reduce_min(action_helper, axis=1).numpy()\n",
    "    print(\"action:\", action)\n",
    "    action_prob = tf.gather(prob, action, axis=1, batch_dims=1).numpy()\n",
    "    action_logits = tf.gather(pred_logits, action, axis=1, batch_dims=1).numpy()\n",
    "    reward_list = []\n",
    "    for i in range(batch_size):\n",
    "        if action[i] == train_labels[i]:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        reward_list.append(reward)\n",
    "    #print(\"reward:\",reward_list)\n",
    "    weight_scale = 1.0 \n",
    "    new_y = collections.OrderedDict([\n",
    "          ('label', pred_action),#predicted action\n",
    "          ('action', action),#actual action\n",
    "          ('reward', reward_list),#comparation between correct action(test_label) and actual action\n",
    "          ('prob', action_prob),\n",
    "          ('logits', action_logits),\n",
    "          ('weight_scale', weight_scale),\n",
    "      ])\n",
    "    return collections.OrderedDict(x=train_features, y=new_y)\n",
    "\n",
    "\n",
    "#bandits_data = softmax_sampling_bandit_data(qnn,0.05)\n",
    "#print(\"----------------------------------------------------------------------\")\n",
    "#print(bandits_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Bandit in kernel-SVC\n",
    "Use bandit data to train the SVC. Mainly change the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kenel Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.utils.loss_functions import KernelLoss\n",
    "from qiskit_machine_learning.kernels import TrainableKernel\n",
    "from typing import Sequence\n",
    "import numpy as np\n",
    "class SVCLoss(KernelLoss):\n",
    "    r\"\"\"\n",
    "    This class provides a kernel loss function for classification tasks by fitting an ``SVC`` model\n",
    "    from scikit-learn. Given training samples, :math:`x_{i}`, with binary labels, :math:`y_{i}`,\n",
    "    and a kernel, :math:`K_{θ}`, parameterized by values, :math:`θ`, the loss is defined as:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        SVCLoss = \\sum_{i} a_i - 0.5 \\sum_{i,j} a_i a_j y_{i} y_{j} K_θ(x_i, x_j)\n",
    "\n",
    "    where :math:`a_i` are the optimal Lagrange multipliers found by solving the standard SVM\n",
    "    quadratic program. Note that the hyper-parameter ``C`` for the soft-margin penalty can be\n",
    "    specified through the keyword args.\n",
    "\n",
    "    Minimizing this loss over the parameters, :math:`θ`, of the kernel is equivalent to maximizing a\n",
    "    weighted kernel alignment, which in turn yields the smallest upper bound to the SVM\n",
    "    generalization error for a given parameterization.\n",
    "\n",
    "    See https://arxiv.org/abs/2105.03406 for further details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            **kwargs: Arbitrary keyword arguments to pass to SVC constructor within\n",
    "                      SVCLoss evaluation.\n",
    "        \"\"\"\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        parameter_values: Sequence[float],\n",
    "        quantum_kernel: TrainableKernel,\n",
    "        data: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "    ) -> float:\n",
    "        # Bind training parameters\n",
    "        quantum_kernel.assign_training_parameters(parameter_values)\n",
    "\n",
    "        # Get estimated kernel matrix\n",
    "        kmatrix = quantum_kernel.evaluate(np.array(data))\n",
    "\n",
    "        # Train a quantum support vector classifier\n",
    "        svc = SVC(kernel=\"precomputed\", **self.kwargs)\n",
    "        svc.fit(kmatrix, labels)\n",
    "\n",
    "        # Get dual coefficients\n",
    "        dual_coefs = svc.dual_coef_[0]\n",
    "\n",
    "        # Get support vectors\n",
    "        support_vecs = svc.support_\n",
    "\n",
    "        # Prune kernel matrix of non-support-vector entries\n",
    "        kmatrix = kmatrix[support_vecs, :][:, support_vecs]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = np.sum(np.abs(dual_coefs)) - (0.5 * (dual_coefs.T @ kmatrix @ dual_coefs))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "from qiskit_algorithms.optimizers import GradientDescent\n",
    "from qiskit_algorithms.optimizers import COBYLA\n",
    "from qiskit_machine_learning.algorithms import NeuralNetworkClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "scoreList = []\n",
    "rewardList = []\n",
    "lossList = []\n",
    "precisionList = []\n",
    "recallList = []\n",
    "def callback_bandit(score,reward,loss,precision,recall):\n",
    "    clear_output(wait=True)\n",
    "    scoreList.append(score)\n",
    "    plt.title(\"Test Score against iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Test Score\")\n",
    "    plt.plot(range(len(scoreList)), scoreList)\n",
    "    plt.show()\n",
    "\n",
    "    rewardList.append(reward)\n",
    "    plt.title(\"Average Reward value against iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.plot(range(len(rewardList)), rewardList)\n",
    "    plt.show()\n",
    "\n",
    "    lossList.append(loss)\n",
    "    plt.title(\"Loss value against iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(range(len(lossList)), lossList)\n",
    "    plt.show()\n",
    "\n",
    "    precisionList.append(precision)\n",
    "    plt.title(\"Precision rate against iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.plot(range(len(precisionList)), precisionList)\n",
    "    plt.show()\n",
    "\n",
    "    recallList.append(recall)\n",
    "    plt.title(\"Recall rate against iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.plot(range(len(recallList)), recallList)\n",
    "    plt.show()\n",
    "\n",
    "def banditLoss(weights):\n",
    "    adhoc_svc.fit_result.x = weights\n",
    "    # epsilon greedy\n",
    "    bandits_data = epsilon_greedy_bandit_data(qnn,0.1)\n",
    "    # falcon\n",
    "    #bandits_data = falcon_bandit_data(train_set,qnn,12,5000)\n",
    "    # softmax sampling\n",
    "    #bandits_data = softmax_sampling_bandit_data(qnn,0.05)\n",
    "    \n",
    "    logits = np.array(bandits_data['y']['logits'])\n",
    "    reward = np.array(bandits_data['y']['reward'])\n",
    "    sum_reward=float(sum(reward))\n",
    "    prob = np.array(bandits_data['y']['prob'])\n",
    "    weight_scale = np.array(bandits_data['y']['weight_scale'])\n",
    "    sample_weights =  weight_scale / prob\n",
    "    #label = np.array(bandits_data['y']['label'])\n",
    "\n",
    "    #cross entropy loss\n",
    "    #loss_function = CrossEntropyLoss()\n",
    "    #loss = loss_function(logits, reward, sample_weights)\n",
    "    #loss_item = float(np.squeeze(loss))\n",
    "    #print(\"Loss:\", loss_item)\n",
    "\n",
    "    #squared error\n",
    "    loss_function = KernelLoss()\n",
    "    loss = loss_function(logits, reward,sample_weights)\n",
    "    loss_item = sum(loss)\n",
    "    #print(\"Loss:\", loss_item)\n",
    "\n",
    "    #squared error\n",
    "    #loss_function = L1Loss_w()\n",
    "    #loss = loss_function(logits,reward,sample_weights)\n",
    "    #loss_item = sum(loss)\n",
    "    print(\"Loss_5:\", loss_item)\n",
    "    print(\"logits:\",logits)\n",
    "    print(\"reward_avg:\",sum(reward)/len(reward))\n",
    "    \n",
    "    print(\"Score:\", qnn.score(test_features, test_labels))\n",
    "    print(\"sum_reward:\",sum_reward)\n",
    "    #print(qnn.fit_result.x)\n",
    "    #print(qnn.weights)\n",
    "    score = qnn.score(test_features, test_labels)\n",
    "    pred_labels = qnn.predict(test_features)\n",
    "    precision = precision_score(test_labels, pred_labels,average=None)\n",
    "    print(f'Precision: {precision}')\n",
    "    recall = recall_score(test_labels, pred_labels,average=None)\n",
    "    print(f'Recall: {recall}')\n",
    "    avg_reward = sum_reward/len(reward)\n",
    "    #callback_bandit(score)\n",
    "    callback_bandit(score,avg_reward,loss_item,precision,recall)\n",
    "    return loss_item\n",
    "\n",
    "optimizer = GradientDescent(maxiter=100, learning_rate=0.01)\n",
    "#optimizer = COBYLA(maxiter=50)\n",
    "fit_result = optimizer.minimize(fun=banditLoss, x0=qnn.weights)\n",
    "#qnn.initial_point = weights['x']\n",
    "#print(qnn.fit_result.x)\n",
    "#print(\"result:\", k)\n",
    "qnn.fit_result.x = fit_result.x\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
